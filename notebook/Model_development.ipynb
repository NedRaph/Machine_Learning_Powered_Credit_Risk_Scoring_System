{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65bf863",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e64996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, confusion_matrix, RocCurveDisplay, average_precision_score)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a645f7e",
   "metadata": {},
   "source": [
    "## 2. Define features, target, train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb72bde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: map to binary (default=1, good=0)\n",
    "df2=pd.read_csv(r\"C:\\Users\\Bonareri\\Machine_Learning_Powered_Credit_Risk_Scoring_System\\data\\clean_df\")\n",
    "df_model = df2.copy()\n",
    "df_model[\"target\"] = (df_model[\"loan_status\"] == \"Charged Off\").astype(int)\n",
    "\n",
    "# Feature lists (based on our EDA selection)\n",
    "cat_cols = [\"home_ownership\",\"verification_status\",\"purpose\",\"addr_state\",\"grade\",\"sub_grade\"]\n",
    "num_cols = [\"annual_inc\",\"emp_length\",\"loan_amnt\",\"term\",\"int_rate\",\"installment\",\n",
    "            \"dti\",\"delinq_2yrs\",\"inq_last_6mths\",\"open_acc\",\"pub_rec\",\"revol_bal\",\n",
    "            \"revol_util\",\"total_acc\"]\n",
    "\n",
    "X = df_model[cat_cols + num_cols]\n",
    "y = df_model[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d3a9e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    85.641518\n",
       "1    14.358482\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.target.value_counts()/ len(df_model) * 100\n",
    "# imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd5a3e",
   "metadata": {},
   "source": [
    "# 3. Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca68695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column transformer: OHE for cats, scale numerics\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e0250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dd0214a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29997, 128), (7500, 128), 0.14358102476914358, 0.1436)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "# Using stratified split to maintain the proportion of target classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e9d33e",
   "metadata": {},
   "source": [
    "# 4. Define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c713ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logreg', 'rf', 'hgb', 'xgb']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = {}\n",
    "\n",
    "# Logistic Regression (interpretable baseline)\n",
    "models[\"logreg\"] = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=None))\n",
    "])\n",
    "\n",
    "# Random Forest (non-linear baseline)\n",
    "models[\"rf\"] = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# HistGradientBoosting (fast, strong tabular baseline)\n",
    "models[\"hgb\"] = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", HistGradientBoostingClassifier(\n",
    "        learning_rate=0.08, max_depth=None, max_leaf_nodes=31,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# XGBoost \n",
    "models[\"xgb\"] = Pipeline(steps=[\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        n_estimators=500, max_depth=6, learning_rate=0.07,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "        eval_metric=\"logloss\", random_state=42, n_jobs=-1,\n",
    "        scale_pos_weight=float((y_train==0).sum()/(y_train==1).sum())  # imbalance\n",
    "    ))\n",
    "])\n",
    "\n",
    "list(models.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d04048cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    y_true  : true labels\n",
    "    y_pred  : predicted labels (0/1)\n",
    "    y_proba : predicted probabilities (for ROC-AUC, PR-AUC)\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    if y_proba is not None:  # use probabilities for ROC-AUC and PR-AUC\n",
    "        roc_auc = roc_auc_score(y_true, y_proba)\n",
    "        pr_auc = average_precision_score(y_true, y_proba)\n",
    "    else:  # fallback to hard predictions\n",
    "        roc_auc = roc_auc_score(y_true, y_pred)\n",
    "        pr_auc = average_precision_score(y_true, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc, pr_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de45141",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a63ee2cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82a0963e",
   "metadata": {},
   "source": [
    "# 5. Train, Evaluate, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b2c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.6517\n",
      "- Precision score: 0.2402\n",
      "- Recall score: 0.6594\n",
      "- F1 score: 0.3522\n",
      "- Roc_auc score: 0.6549\n",
      "- Average precision score: 0.2073\n",
      "----------------------------------\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.6585\n",
      "- Precision score: 0.2450\n",
      "- Recall score: 0.6620\n",
      "- F1 score: 0.3577\n",
      "- Roc_auc score: 0.6600\n",
      "- Average precision score: 0.2107\n",
      "===================================\n",
      "\n",
      "\n",
      "Hist Gradient Boosting Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.8638\n",
      "- Precision score: 0.8746\n",
      "- Recall score: 0.0599\n",
      "- F1 score: 0.1121\n",
      "- Roc_auc score: 0.5292\n",
      "- Average precision score: 0.1874\n",
      "----------------------------------\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.8571\n",
      "- Precision score: 0.5472\n",
      "- Recall score: 0.0269\n",
      "- F1 score: 0.0513\n",
      "- Roc_auc score: 0.5116\n",
      "- Average precision score: 0.1545\n",
      "===================================\n",
      "\n",
      "\n",
      "Random Forest Classifier\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.9994\n",
      "- Precision score: 0.9991\n",
      "- Recall score: 0.9965\n",
      "- F1 score: 0.9978\n",
      "- Roc_auc score: 0.9982\n",
      "- Average precision score: 0.9961\n",
      "----------------------------------\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.8535\n",
      "- Precision score: 0.4345\n",
      "- Recall score: 0.0678\n",
      "- F1 score: 0.1173\n",
      "- Roc_auc score: 0.5265\n",
      "- Average precision score: 0.1633\n",
      "===================================\n",
      "\n",
      "\n",
      "XGBClassifier\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.9008\n",
      "- Precision score: 0.5969\n",
      "- Recall score: 0.9508\n",
      "- F1 score: 0.7334\n",
      "- Roc_auc score: 0.9216\n",
      "- Average precision score: 0.5746\n",
      "----------------------------------\n",
      "Model performance for Training set\n",
      "- Accuracy score: 0.7560\n",
      "- Precision score: 0.2711\n",
      "- Recall score: 0.4141\n",
      "- F1 score: 0.3277\n",
      "- Roc_auc score: 0.6137\n",
      "- Average precision score: 0.1964\n",
      "===================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=2000, class_weight=\"balanced\", n_jobs=None),\n",
    "    \"Hist Gradient Boosting Classifier\": HistGradientBoostingClassifier(#learning_rate=0.08, max_depth=None, max_leaf_nodes=31,\n",
    "        random_state=42),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(n_estimators=400, max_depth=None, min_samples_leaf=2,\n",
    "        class_weight=\"balanced\", random_state=42, n_jobs=-1),\n",
    "    \"XGBClassifier\": XGBClassifier(n_estimators=500, max_depth=6, learning_rate=0.07,\n",
    "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
    "        eval_metric=\"logloss\", random_state=42, n_jobs=-1,\n",
    "        scale_pos_weight=float((y_train==0).sum()/(y_train==1).sum())) \n",
    "    \n",
    "}\n",
    "model_list = []\n",
    "metrics_list =[]\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train) # Train model\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate Train and Test dataset\n",
    "    model_train_accuracy, model_train_precision, model_train_recall, model_train_f1, model_train_roc_auc, model_train_pr_auc = evaluate_model(y_train, y_train_pred)\n",
    "\n",
    "    model_test_accuracy, model_test_precision, model_test_recall, model_test_f1, model_test_roc_auc, model_test_pr_auc = evaluate_model(y_test, y_test_pred)\n",
    "\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print('Model performance for Training set')\n",
    "    print(\"- Accuracy score: {:.4f}\".format(model_train_accuracy))\n",
    "    print(\"- Precision score: {:.4f}\".format(model_train_precision))\n",
    "    print(\"- Recall score: {:.4f}\".format(model_train_recall))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"- Roc_auc score: {:.4f}\".format(model_train_roc_auc))\n",
    "    print(\"- Average precision score: {:.4f}\".format(model_train_pr_auc))\n",
    "\n",
    "    print('----------------------------------')\n",
    "    \n",
    "    print('Model performance for Testing set')\n",
    "    print(\"- Accuracy score: {:.4f}\".format(model_test_accuracy))\n",
    "    print(\"- Precision score: {:.4f}\".format(model_test_precision))\n",
    "    print(\"- Recall score: {:.4f}\".format(model_test_recall))\n",
    "    print(\"- F1 score: {:.4f}\".format(model_test_f1))\n",
    "    print(\"- Roc_auc score: {:.4f}\".format(model_test_roc_auc))\n",
    "    print(\"- Average precision score: {:.4f}\".format(model_test_pr_auc))\n",
    "    metrics_list.append(model_test_accuracy)\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a93d4",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "1. Logistic Regression\n",
    "Train vs Test:\n",
    "Train accuracy: 0.65 | Test accuracy: 0.66 - very similar No overfitting.\n",
    "Precision is low (0.24) but recall is fairly high (0.66). The model catches many positives but with many false alarms.\n",
    "F1 is low (0.35) because precision drags it down.\n",
    "ROC-AUC 0.65 - slightly better than random guessing, but weak.\n",
    "\n",
    "Interpretation: Logistic regression is providing balanced but weak baseline. It’s stable but doesn’t separate classes well.\n",
    "\n",
    "2. HistGradientBoosting\n",
    "Train vs Test:\n",
    "Accuracy: 0.86 (train), 0.85 (test) - Generalizes well.\n",
    "But recall is terrible (0.06 train, 0.02 test) - It’s predicting almost all negatives.\n",
    "Precision is misleadingly high on train (0.87) but collapses on test (0.54).\n",
    "ROC-AUC 0.52 (train/test) - basically random performance.\n",
    "\n",
    "Interpretation: The model is too conservative, it avoids predicting positives, so recall suffers badly.\n",
    "\n",
    "3. Random Forest\n",
    "Train vs Test:\n",
    "Train accuracy: 0.999 - Clear overfitting.\n",
    "Test accuracy: 0.85, but recall only 0.07 - terrible recall.\n",
    "ROC-AUC 0.52 on test - random.\n",
    "\n",
    "Interpretation: Random forest memorized the training set (overfit) but fails to generalize.\n",
    "\n",
    "4. XGBoost\n",
    "Train vs Test:\n",
    "Train accuracy: 0.90 | Test accuracy: 0.75 - Some overfitting.\n",
    "Recall on train: 0.95 - excellent, but test recall: 0.41 - much lower.\n",
    "Precision test 0.27 - a lot of false positives.\n",
    "ROC-AUC test 0.61 - weak, but better than RF/HGB.\n",
    "\n",
    "Interpretation: This is our best model so far because it at least captures recall, but it still overfits and loses power on test.\n",
    "\n",
    "Key Takeaway: Logistic Regression is our best algorithm, we will pick it and optimize.\n",
    "\n",
    "Since we are working on credit scoring, catching defaults (positives) is more important. optimize for recall, not accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f611484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b51be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "058f83b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Baseline Model (Threshold=0.5) ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77      6423\n",
      "           1       0.24      0.66      0.36      1077\n",
      "\n",
      "    accuracy                           0.66      7500\n",
      "   macro avg       0.58      0.66      0.56      7500\n",
      "weighted avg       0.82      0.66      0.71      7500\n",
      "\n",
      "\n",
      "==== After Lowering Threshold to 0.3 ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.25      0.39      6423\n",
      "           1       0.17      0.93      0.29      1077\n",
      "\n",
      "    accuracy                           0.35      7500\n",
      "   macro avg       0.56      0.59      0.34      7500\n",
      "weighted avg       0.84      0.35      0.38      7500\n",
      "\n",
      "\n",
      "==== After SMOTE Oversampling ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.65      0.76      6423\n",
      "           1       0.24      0.66      0.35      1077\n",
      "\n",
      "    accuracy                           0.65      7500\n",
      "   macro avg       0.58      0.66      0.56      7500\n",
      "weighted avg       0.82      0.65      0.71      7500\n",
      "\n",
      "\n",
      "==== After Hyperparameter Tuning for Recall ====\n",
      "Best C: 0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77      6423\n",
      "           1       0.25      0.66      0.36      1077\n",
      "\n",
      "    accuracy                           0.66      7500\n",
      "   macro avg       0.58      0.66      0.56      7500\n",
      "weighted avg       0.82      0.66      0.71      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Baseline Logistic Regression\n",
    "# ----------------------------\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_baseline = logreg.predict(X_test)\n",
    "print(\"==== Baseline Model (Threshold=0.5) ====\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Adjust Threshold\n",
    "# ----------------------------\n",
    "y_probs = logreg.predict_proba(X_test)[:, 1]\n",
    "y_pred_thresh = (y_probs >= 0.3).astype(int)   # lower threshold\n",
    "\n",
    "print(\"\\n==== After Lowering Threshold to 0.3 ====\")\n",
    "print(classification_report(y_test, y_pred_thresh))\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Handle Imbalance with SMOTE\n",
    "# ----------------------------\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "logreg_smote = LogisticRegression(max_iter=2000, random_state=42)\n",
    "logreg_smote.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred_smote = logreg_smote.predict(X_test)\n",
    "print(\"\\n==== After SMOTE Oversampling ====\")\n",
    "print(classification_report(y_test, y_pred_smote))\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Hyperparameter Tuning for Recall\n",
    "# ----------------------------\n",
    "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\",\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "best_logreg = grid.best_estimator_\n",
    "\n",
    "y_pred_best = best_logreg.predict(X_test)\n",
    "print(\"\\n==== After Hyperparameter Tuning for Recall ====\")\n",
    "print(\"Best C:\", grid.best_params_[\"C\"])\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72e1cd",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "1. Baseline (Threshold = 0.5)\n",
    "Precision (class 1): 0.24\n",
    "Recall (class 1): 0.66\n",
    "F1 (class 1): 0.36\n",
    "Accuracy: 0.66\n",
    "\n",
    "This is actually decent recall for the minority class (66%). Provides balance (not too many false alarms), but precision is low — lots of false positives.\n",
    "\n",
    "2. Lowering Threshold to 0.3\n",
    "\n",
    "Precision (class 1): 0.17 → dropped a lot\n",
    "\n",
    "Recall (class 1): 0.93 → shot way up\n",
    "\n",
    "F1 (class 1): 0.29 → dropped\n",
    "\n",
    "By lowering the threshold, the model predicts “positive” much more often, so it catches nearly all true positives (93% recall), but at the cost of predicting many false positives (precision collapse).\n",
    "This is great if recall is critical (e.g., catching almost all risky clients), but business costs of false positives matter. We would actually pick this\n",
    "\n",
    "3. After SMOTE Oversampling\n",
    "Precision (class 1): 0.24\n",
    "Recall (class 1): 0.66\n",
    "F1 (class 1): 0.35\n",
    "This is basically the same as baseline, which tells us SMOTE didn’t add much value here.\n",
    "\n",
    "4. After Hyperparameter Tuning (Best C = 0.01)\n",
    "Precision (class 1): 0.25\n",
    "Recall (class 1): 0.66\n",
    "F1 (class 1): 0.36\n",
    "\n",
    "Almost identical to baseline.\n",
    "The best regularization (C=0.01) slightly stabilizes the model, but it doesn’t fundamentally change performance \n",
    "\n",
    "Key takeaway:\n",
    "We want maximum recall (dont miss risky borrowers), but sacrifice precision.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd48419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
